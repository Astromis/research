{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99147c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy import sparse\n",
    "from math import ceil\n",
    "from textfab import Conveyer\n",
    "import re\n",
    "from scipy import sparse as sc \n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6058367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f39f5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest \n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59748c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd99a9c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "# model.cuda()  # uncomment it if you have a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5beb1a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def shuffle(matrix):\n",
    "    index = np.arange(matrix.shape[0])   \n",
    "    np.random.shuffle(index)    \n",
    "    return matrix[index, :]\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "def preprocess(corp):\n",
    "    con = Conveyer(['remove_punct', \"lower_string\", \"swap_enter_to_space\", \"collapse_spaces\"])\n",
    "    corp = list(map(lambda x: re.sub(r\"<emoji>.+</emoji>\", \"\", x), corp))\n",
    "    corp = list(filter(lambda x: \"<no text>\" not in x, corp))\n",
    "    corp = list(map(lambda x:re.sub(\"[A-Za-z]+\", '', x), corp))\n",
    "    corp = con.start(corp)\n",
    "    corp = list(filter(lambda x: len(x) > 2, corp))\n",
    "    return corp\n",
    "\n",
    "def block_dataset(negative_vectors, positive_vectors):\n",
    "    train_sets = []\n",
    "    start = 0\n",
    "    out_num = positive_vectors.shape[0]\n",
    "    norm_num = negative_vectors.shape[0]\n",
    "    while norm_num != 0:\n",
    "        end = norm_num % out_num\n",
    "        if end != 0 :\n",
    "            if (end / out_num) > 0.5:\n",
    "                train_sets.append( (negative_vectors[start:start + end], positive_vectors[start:start + end]) )\n",
    "            start += end\n",
    "            norm_num -= end\n",
    "        else:\n",
    "            end += out_num\n",
    "            train_sets.append( (negative_vectors[start:start + end], positive_vectors) )\n",
    "            norm_num -= out_num\n",
    "            start += end\n",
    "    return train_sets\n",
    "\n",
    "def get_metrics(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average=None), recall_score(y_true, y_pred, average=None), f1_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e45398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/presuicidal_signals_dataset_twitter.csv\", sep=\"|\")\n",
    "data.text = data.text.astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328d1e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# vectorizer.fit(data.text.to_list())\n",
    "vectorizer.fit( preprocess(data.text.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64a01035",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_texts = preprocess( data[data.label == 5].text.to_list())\n",
    "outlier_texts = preprocess(data[data.label.isin([1,2])].text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "518115be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f546fc73cf45ac8674d8a6e7ae619e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Normal vectorization:   0%|          | 0/26389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c26e43eab04725ab8bcd245f6b1d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Outlier vectorization:   0%|          | 0/4934 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normal_bert_vectors = [embed_bert_cls(x, model, tokenizer) for x in tqdm(normal_texts, desc=\"Normal vectorization\")]\n",
    "outlier_bert_vectors = [embed_bert_cls(x, model, tokenizer) for x in tqdm(outlier_texts, desc=\"Outlier vectorization\")]\n",
    "normal_bert_vectors = np.vstack(normal_bert_vectors)\n",
    "outlier_bert_vectors = np.vstack(outlier_bert_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33dcea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_count_vectors = vectorizer.transform(normal_texts)\n",
    "outlier_count_vectors = vectorizer.transform(outlier_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "037591eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = []\n",
    "f1_macro = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cfb807",
   "metadata": {},
   "source": [
    "# Outlier detection methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d128e7",
   "metadata": {},
   "source": [
    "## Count based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d975494a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394193659b4e46f5986b0099b4da8d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/astromis/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(EXPERIMENT_NUM)):\n",
    "    normal_count_vectors = shuffle(normal_count_vectors)\n",
    "\n",
    "    normal_vectors_test = normal_count_vectors[: outlier_count_vectors.shape[0]]\n",
    "    normal_vectors = normal_count_vectors[outlier_count_vectors.shape[0] : ]\n",
    "\n",
    "    test_vectors = sc.vstack([normal_vectors_test, outlier_count_vectors])\n",
    "    test_label = np.vstack([np.ones((normal_vectors_test.shape[0], 1)),\n",
    "                           -1 * np.ones((normal_vectors_test.shape[0], 1))])\n",
    "\n",
    "    clf_if = IsolationForest()\n",
    "    clf_lof = LocalOutlierFactor(novelty=True)\n",
    "    clf_svm = OneClassSVM(kernel=\"rbf\", nu=outlier_count_vectors.shape[0] / normal_vectors.shape[0], gamma=1e-6)\n",
    "\n",
    "\n",
    "    clf_if.fit(normal_vectors)\n",
    "    m = get_metrics(test_label, clf_if.predict(test_vectors))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Isolation Forest\", \"Count\", \"0\", *zero_class))\n",
    "    report.append((\"Isolation Forest\", \"Count\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Isolation Forest\", \"Count\", f1_score(test_label, clf_if.predict(test_vectors), average=\"macro\")) )\n",
    "    \n",
    "    clf_lof.fit(normal_vectors)\n",
    "    m = get_metrics(test_label, clf_lof.predict(test_vectors))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Local Outlier Factor\", \"Count\", \"0\", *zero_class))\n",
    "    report.append((\"Local Outlier Factor\", \"Count\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Local Outlier Factor\", \"Count\", f1_score(test_label, clf_lof.predict(test_vectors), average=\"macro\")))\n",
    "                  \n",
    "    clf_svm.fit(normal_vectors)\n",
    "    m = get_metrics(test_label, clf_svm.predict(test_vectors))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"OneClassSVM\", \"Count\", \"0\", *zero_class))\n",
    "    report.append((\"OneClassSVM\", \"Count\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"OneClassSVM\", \"Count\", f1_score(test_label, clf_svm.predict(test_vectors), average=\"macro\"))) \n",
    "                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caa29df",
   "metadata": {},
   "source": [
    "## BERT based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78ea6c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a4695e83644355bcc7f857c1bf3875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(EXPERIMENT_NUM)):\n",
    "\n",
    "    np.random.shuffle(normal_bert_vectors)\n",
    "\n",
    "    normal_vectors_test = normal_bert_vectors[: outlier_bert_vectors.shape[0]]\n",
    "    normal_vectors = normal_bert_vectors[outlier_bert_vectors.shape[0] : ]\n",
    "\n",
    "    test_vectors = np.vstack([normal_vectors_test, outlier_bert_vectors])\n",
    "    test_label = np.vstack([np.ones((normal_vectors_test.shape[0], 1)),\n",
    "                           -1 * np.ones((normal_vectors_test.shape[0], 1))])\n",
    "\n",
    "    clf_if = IsolationForest()\n",
    "    clf_lof = LocalOutlierFactor(novelty=True)\n",
    "    clf_svm = OneClassSVM(kernel=\"rbf\", nu=outlier_count_vectors.shape[0] / normal_vectors.shape[0], gamma=1e-6)\n",
    "\n",
    "\n",
    "    clf_if.fit(normal_vectors)\n",
    "    m = get_metrics(test_label, clf_if.predict(test_vectors))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Isolation Forest\", \"BERT\", \"0\", *zero_class))\n",
    "    report.append((\"Isolation Forest\", \"BERT\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Isolation Forest\", \"BERT\", f1_score(test_label, clf_if.predict(test_vectors), average=\"macro\")))\n",
    "    \n",
    "    clf_lof.fit(normal_vectors)\n",
    "    m = get_metrics(test_label, clf_lof.predict(test_vectors))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Local Outlier Factor\", \"BERT\", \"0\", *zero_class))\n",
    "    report.append((\"Local Outlier Factor\", \"BERT\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Local Outlier Factor\", \"BERT\", f1_score(test_label, clf_lof.predict(test_vectors), average=\"macro\")))\n",
    "\n",
    "    clf_svm.fit(normal_vectors)\n",
    "    m = get_metrics(test_label, clf_svm.predict(test_vectors))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"OneClassSVM\", \"BERT\", \"0\", *zero_class))\n",
    "    report.append((\"OneClassSVM\", \"BERT\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"OneClassSVM\", \"BERT\", f1_score(test_label, clf_svm.predict(test_vectors), average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d31f77",
   "metadata": {},
   "source": [
    "# Traditional methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebc9d4",
   "metadata": {},
   "source": [
    "## Count based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87dde8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = sc.vstack([normal_count_vectors, outlier_count_vectors])\n",
    "label = np.vstack([np.zeros((normal_count_vectors.shape[0], 1)),\n",
    "                    np.ones((outlier_count_vectors.shape[0], 1))]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "860136cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=[0,1], y=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b309247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354ce462f38647f9aba7500dcd45ebe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:38:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:38:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:39:22] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:39:49] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:40:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:40:43] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:41:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:41:37] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:42:06] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:42:34] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(EXPERIMENT_NUM)):\n",
    "    x_tr, x_ts, y_tr, y_ts = train_test_split(vectors, label)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, C=1, class_weight={0:class_weights[0], 1:class_weights[1]})\n",
    "    clf.fit(x_tr, y_tr, )\n",
    "    m = get_metrics(y_ts, clf.predict(x_ts))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Logistic Regression\", \"Count\", \"0\", *zero_class))\n",
    "    report.append((\"Logistic Regression\", \"Count\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Logistic Regression\", \"Count\", f1_score(y_ts, clf.predict(x_ts), average=\"macro\")))\n",
    "    \n",
    "    clf = XGBClassifier(scale_pos_weight=class_weights[1])\n",
    "    clf.fit(x_tr, y_tr,)\n",
    "    m = get_metrics(y_ts, clf.predict(x_ts))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"XGBoost\", \"Count\", \"0\", *zero_class))\n",
    "    report.append((\"XGBoost\", \"Count\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"XGBoost\", \"Count\", f1_score(y_ts, clf.predict(x_ts), average=\"macro\")))\n",
    "    \n",
    "    clf = RandomForestClassifier(n_jobs=5, class_weight={0:class_weights[0], 1:class_weights[1]} )\n",
    "    clf.fit(x_tr, y_tr, )\n",
    "    m = get_metrics(y_ts, clf.predict(x_ts))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Random Forest\", \"Count\", \"0\", *zero_class))\n",
    "    report.append((\"Random Forest\", \"Count\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Random Forest\", \"Count\", f1_score(y_ts, clf.predict(x_ts), average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95277ab",
   "metadata": {},
   "source": [
    "## BERT based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3a58941",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = np.vstack([normal_bert_vectors, outlier_bert_vectors])\n",
    "label = np.vstack([np.zeros((normal_bert_vectors.shape[0], 1)),\n",
    "                    np.ones((outlier_bert_vectors.shape[0], 1))]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d56e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=[0,1], y=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5648446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526a9c9e74b3437cba142e77bacbc9e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:43:02] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:43:35] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:09] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:44:42] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:16] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:45:50] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:23] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:46:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:47:29] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/astromis/anaconda3/lib/python3.9/site-packages/xgboost/sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:48:03] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(EXPERIMENT_NUM)): \n",
    "    x_tr, x_ts, y_tr, y_ts = train_test_split(vectors, label)\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000, C=1, class_weight={0:class_weights[0], 1:class_weights[1]})\n",
    "    clf.fit(x_tr, y_tr, )\n",
    "    m = get_metrics(y_ts, clf.predict(x_ts))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Logistic Regression\", \"BERT\", \"0\", *zero_class))\n",
    "    report.append((\"Logistic Regression\", \"BERT\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Logistic Regression\", \"BERT\", f1_score(y_ts, clf.predict(x_ts), average=\"macro\")))\n",
    "    \n",
    "    clf = XGBClassifier(scale_pos_weight=class_weights[1])\n",
    "    clf.fit(x_tr, y_tr,)\n",
    "    m = get_metrics(y_ts, clf.predict(x_ts))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"XGBoost\", \"BERT\", \"0\", *zero_class))\n",
    "    report.append((\"XGBoost\", \"BERT\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"XGBoost\", \"BERT\", f1_score(y_ts, clf.predict(x_ts), average=\"macro\")))\n",
    "    \n",
    "    clf = RandomForestClassifier(n_jobs=5, class_weight={0:class_weights[0], 1:class_weights[1]} )\n",
    "    clf.fit(x_tr, y_tr, )\n",
    "    m = get_metrics(y_ts, clf.predict(x_ts))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"Random Forest\", \"BERT\", \"0\", *zero_class))\n",
    "    report.append((\"Random Forest\", \"BERT\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"Random Forest\", \"BERT\", f1_score(y_ts, clf.predict(x_ts), average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c6a051",
   "metadata": {},
   "source": [
    "# Ensable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7582c825",
   "metadata": {},
   "source": [
    "## CountBased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fb386e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad403134c27e4a948e6109f71bbd698f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(EXPERIMENT_NUM)):\n",
    "    outlier_count_vectors = shuffle(outlier_count_vectors)\n",
    "    normal_count_vectors = shuffle(normal_count_vectors)\n",
    "\n",
    "    outlier_vectors_count_train = outlier_count_vectors[:ceil(0.77 * outlier_count_vectors.shape[0])]\n",
    "    outlier_vectors_count_test = outlier_count_vectors[ceil(0.77 * outlier_count_vectors.shape[0]):]\n",
    "\n",
    "    normal_vectors_count_train = normal_count_vectors[:ceil(0.77 * normal_count_vectors.shape[0])]\n",
    "    normal_vectors_count_test = normal_count_vectors[ceil(0.77 * normal_count_vectors.shape[0]):]\n",
    "\n",
    "    models = []\n",
    "    for normal, outlier  in block_dataset(normal_vectors_count_train, outlier_vectors_count_train):\n",
    "        vectors = sparse.vstack([normal, outlier])#np\n",
    "        label = np.hstack([np.zeros((normal.shape[0])),\n",
    "                        np.ones((outlier.shape[0]))])\n",
    "        x_tr, x_ts, y_tr, y_ts = train_test_split(vectors, label, train_size=0.95)\n",
    "        clf = LogisticRegression(max_iter = 1000, C=1)\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        #print(classification_report(y_ts,clf.predict(x_ts)))\n",
    "        models.append(clf)\n",
    "\n",
    "    vectors_train = sparse.vstack([normal_vectors_count_train, outlier_vectors_count_train]) # np\n",
    "    features = np.hstack([x.predict_proba(vectors_train) for x in models])\n",
    "\n",
    "    label = np.hstack([np.zeros((normal_vectors_count_train.shape[0])),\n",
    "                        np.ones((outlier_vectors_count_train.shape[0]))])\n",
    "\n",
    "    #x_tr, x_ts, y_tr, y_ts = train_test_split(features, label, train_size=0.77)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=[0,1], y=label)\n",
    "\n",
    "    clf = LogisticRegression(max_iter = 1000, C=1, class_weight={0:class_weights[0], 1:class_weights[1]})\n",
    "    clf.fit(features, label)\n",
    "    \n",
    "    vectors_test = sparse.vstack([normal_vectors_count_test, outlier_vectors_count_test])\n",
    "    features = np.hstack([x.predict_proba(vectors_test) for x in models])\n",
    "\n",
    "    label = np.hstack([np.zeros((normal_vectors_count_test.shape[0])),\n",
    "                        np.ones((outlier_vectors_count_test.shape[0]))])\n",
    "    \n",
    "    m = get_metrics(label, clf.predict(features))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"LogReg Stack\", \"Count\", \"0\", *zero_class))\n",
    "    report.append((\"LogReg Stack\", \"Count\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"LogReg Stack\", \"Count\", f1_score(label, clf.predict(features), average=\"macro\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a1c292",
   "metadata": {},
   "source": [
    "## BERT based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f35a338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f59752ee3f4e7995d4c77ea9903695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(EXPERIMENT_NUM)):\n",
    "    np.random.shuffle(normal_bert_vectors)\n",
    "    np.random.shuffle(outlier_bert_vectors)\n",
    "    \n",
    "    outlier_vectors_bert_train = outlier_bert_vectors[:ceil(0.77 * outlier_bert_vectors.shape[0])]\n",
    "    outlier_vectors_bert_test = outlier_bert_vectors[ceil(0.77 * outlier_bert_vectors.shape[0]):]\n",
    "\n",
    "    normal_vectors_bert_train = normal_bert_vectors[:ceil(0.77 * normal_bert_vectors.shape[0])]\n",
    "    normal_vectors_bert_test = normal_bert_vectors[ceil(0.77 * normal_bert_vectors.shape[0]):]\n",
    "\n",
    "    models = []\n",
    "    for normal, outlier  in block_dataset(normal_vectors_bert_train, outlier_vectors_bert_train):\n",
    "        vectors = np.vstack([normal, outlier])#np\n",
    "        label = np.hstack([np.zeros((normal.shape[0])),\n",
    "                        np.ones((outlier.shape[0]))])\n",
    "        x_tr, x_ts, y_tr, y_ts = train_test_split(vectors, label, train_size=0.95)\n",
    "        clf = LogisticRegression(max_iter = 1000, C=1)\n",
    "        clf.fit(x_tr, y_tr)\n",
    "        #print(classification_report(y_ts,clf.predict(x_ts)))\n",
    "        models.append(clf)\n",
    "\n",
    "    vectors_train = np.vstack([normal_vectors_bert_train, outlier_vectors_bert_train]) # np\n",
    "    features = np.hstack([x.predict_proba(vectors_train) for x in models])\n",
    "\n",
    "    label = np.hstack([np.zeros((normal_vectors_bert_train.shape[0])),\n",
    "                        np.ones((outlier_vectors_bert_train.shape[0]))])\n",
    "\n",
    "    #x_tr, x_ts, y_tr, y_ts = train_test_split(features, label, train_size=0.77)\n",
    "\n",
    "    class_weights = compute_class_weight('balanced', classes=[0,1], y=label)\n",
    "\n",
    "    clf = LogisticRegression(max_iter = 1000, C=1, class_weight={0:class_weights[0], 1:class_weights[1]})\n",
    "    clf.fit(features, label)\n",
    "    \n",
    "    vectors_test = np.vstack([normal_vectors_bert_test, outlier_vectors_bert_test])\n",
    "    features = np.hstack([x.predict_proba(vectors_test) for x in models])\n",
    "\n",
    "    label = np.hstack([np.zeros((normal_vectors_bert_test.shape[0])),\n",
    "                        np.ones((outlier_vectors_bert_test.shape[0]))])\n",
    "    \n",
    "    m = get_metrics(label, clf.predict(features))\n",
    "    zero_class = tuple([x[0] for x in m])\n",
    "    first_calss = tuple([x[1] for x in m])\n",
    "    report.append((\"LogReg Stack\", \"BERT\", \"0\", *zero_class))\n",
    "    report.append((\"LogReg Stack\", \"BERT\", \"1\", *first_calss))\n",
    "    f1_macro.append( (\"LogReg Stack\", \"BERT\", f1_score(label, clf.predict(features), average=\"macro\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6610cec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dump(report, open(\"results.json\", 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "36c55e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.DataFrame(report, columns=[\"method\", \"vec\", \"class\", \"precision\", \"recall\", \"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d100f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>method</th>\n",
       "      <th>vec</th>\n",
       "      <th>class</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Isolation Forest</td>\n",
       "      <td>Count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Isolation Forest</td>\n",
       "      <td>Count</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Local Outlier Factor</td>\n",
       "      <td>Count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.501734</td>\n",
       "      <td>0.996960</td>\n",
       "      <td>0.667526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Local Outlier Factor</td>\n",
       "      <td>Count</td>\n",
       "      <td>1</td>\n",
       "      <td>0.765625</td>\n",
       "      <td>0.009931</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OneClassSVM</td>\n",
       "      <td>Count</td>\n",
       "      <td>0</td>\n",
       "      <td>0.623984</td>\n",
       "      <td>0.373328</td>\n",
       "      <td>0.467157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>LogReg Stack</td>\n",
       "      <td>BERT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.377310</td>\n",
       "      <td>0.774250</td>\n",
       "      <td>0.507368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>LogReg Stack</td>\n",
       "      <td>BERT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.950473</td>\n",
       "      <td>0.762070</td>\n",
       "      <td>0.845908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>LogReg Stack</td>\n",
       "      <td>BERT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.382114</td>\n",
       "      <td>0.787478</td>\n",
       "      <td>0.514549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>LogReg Stack</td>\n",
       "      <td>BERT</td>\n",
       "      <td>0</td>\n",
       "      <td>0.947562</td>\n",
       "      <td>0.762234</td>\n",
       "      <td>0.844854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>LogReg Stack</td>\n",
       "      <td>BERT</td>\n",
       "      <td>1</td>\n",
       "      <td>0.378285</td>\n",
       "      <td>0.774250</td>\n",
       "      <td>0.508249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows  6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   method    vec class  precision    recall        f1\n",
       "0        Isolation Forest  Count     0   0.000000  0.000000  0.000000\n",
       "1        Isolation Forest  Count     1   0.500000  1.000000  0.666667\n",
       "2    Local Outlier Factor  Count     0   0.501734  0.996960  0.667526\n",
       "3    Local Outlier Factor  Count     1   0.765625  0.009931  0.019608\n",
       "4             OneClassSVM  Count     0   0.623984  0.373328  0.467157\n",
       "..                    ...    ...   ...        ...       ...       ...\n",
       "275          LogReg Stack   BERT     1   0.377310  0.774250  0.507368\n",
       "276          LogReg Stack   BERT     0   0.950473  0.762070  0.845908\n",
       "277          LogReg Stack   BERT     1   0.382114  0.787478  0.514549\n",
       "278          LogReg Stack   BERT     0   0.947562  0.762234  0.844854\n",
       "279          LogReg Stack   BERT     1   0.378285  0.774250  0.508249\n",
       "\n",
       "[280 rows x 6 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1ed0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    " a = t.groupby([\"method\", \"vec\", \"class\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7bd919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3b32fdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">precision</th>\n",
       "      <th colspan=\"2\" halign=\"left\">recall</th>\n",
       "      <th colspan=\"2\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th>vec</th>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Isolation Forest</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT</th>\n",
       "      <th>0</th>\n",
       "      <td>0.558</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count</th>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Local Outlier Factor</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT</th>\n",
       "      <th>0</th>\n",
       "      <td>0.301</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.497</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.979</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count</th>\n",
       "      <th>0</th>\n",
       "      <td>0.502</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.997</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.768</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">LogReg Stack</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT</th>\n",
       "      <th>0</th>\n",
       "      <td>0.948</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.382</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count</th>\n",
       "      <th>0</th>\n",
       "      <td>0.909</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.313</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Logistic Regression</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT</th>\n",
       "      <th>0</th>\n",
       "      <td>0.947</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.391</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count</th>\n",
       "      <th>0</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.693</td>\n",
       "      <td>0.302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.292</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.384</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">OneClassSVM</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT</th>\n",
       "      <th>0</th>\n",
       "      <td>0.486</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.577</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.512</td>\n",
       "      <td>0.108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.491</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count</th>\n",
       "      <th>0</th>\n",
       "      <td>0.621</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.552</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.644</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">Random Forest</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT</th>\n",
       "      <th>0</th>\n",
       "      <td>0.856</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.920</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count</th>\n",
       "      <th>0</th>\n",
       "      <td>0.856</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.991</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.671</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">XGBoost</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">BERT</th>\n",
       "      <th>0</th>\n",
       "      <td>0.899</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.915</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.548</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Count</th>\n",
       "      <th>0</th>\n",
       "      <td>0.899</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.911</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.516</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.442</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.476</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 precision       recall          f1      \n",
       "                                      mean   std   mean   std  mean   std\n",
       "method               vec   class                                         \n",
       "Isolation Forest     BERT  0         0.558 0.351  0.000 0.000 0.001 0.000\n",
       "                           1         0.500 0.000  1.000 0.000 0.667 0.000\n",
       "                     Count 0         0.000 0.000  0.000 0.000 0.000 0.000\n",
       "                           1         0.500 0.000  1.000 0.000 0.667 0.000\n",
       "Local Outlier Factor BERT  0         0.301 0.022  0.009 0.001 0.017 0.001\n",
       "                           1         0.497 0.001  0.979 0.003 0.659 0.001\n",
       "                     Count 0         0.502 0.000  0.997 0.000 0.668 0.000\n",
       "                           1         0.768 0.033  0.011 0.002 0.021 0.004\n",
       "LogReg Stack         BERT  0         0.948 0.002  0.765 0.005 0.847 0.003\n",
       "                           1         0.382 0.004  0.777 0.011 0.512 0.004\n",
       "                     Count 0         0.909 0.007  0.754 0.018 0.824 0.013\n",
       "                           1         0.313 0.021  0.598 0.030 0.410 0.024\n",
       "Logistic Regression  BERT  0         0.947 0.002  0.774 0.004 0.852 0.002\n",
       "                           1         0.391 0.010  0.770 0.006 0.518 0.009\n",
       "                     Count 0         0.923 0.035  0.632 0.299 0.693 0.302\n",
       "                           1         0.292 0.078  0.656 0.190 0.384 0.065\n",
       "OneClassSVM          BERT  0         0.486 0.015  0.577 0.220 0.512 0.108\n",
       "                           1         0.491 0.013  0.401 0.202 0.414 0.130\n",
       "                     Count 0         0.621 0.006  0.374 0.001 0.467 0.001\n",
       "                           1         0.552 0.002  0.772 0.007 0.644 0.003\n",
       "Random Forest        BERT  0         0.856 0.004  0.994 0.001 0.920 0.002\n",
       "                           1         0.770 0.032  0.112 0.007 0.196 0.010\n",
       "                     Count 0         0.856 0.005  0.991 0.001 0.918 0.003\n",
       "                           1         0.671 0.028  0.098 0.012 0.171 0.019\n",
       "XGBoost              BERT  0         0.899 0.004  0.931 0.003 0.915 0.001\n",
       "                           1         0.548 0.018  0.445 0.016 0.491 0.013\n",
       "                     Count 0         0.899 0.004  0.923 0.003 0.911 0.002\n",
       "                           1         0.516 0.016  0.442 0.009 0.476 0.009"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ea5cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1m = pd.DataFrame(f1_macro, columns=[\"method\", \"vec\", \"f1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec07eae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th>vec</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Isolation Forest</th>\n",
       "      <th>BERT</th>\n",
       "      <td>0.334</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.333</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Local Outlier Factor</th>\n",
       "      <th>BERT</th>\n",
       "      <td>0.338</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.344</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">LogReg Stack</th>\n",
       "      <th>BERT</th>\n",
       "      <td>0.680</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.617</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Logistic Regression</th>\n",
       "      <th>BERT</th>\n",
       "      <td>0.685</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.538</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">OneClassSVM</th>\n",
       "      <th>BERT</th>\n",
       "      <td>0.463</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.555</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Random Forest</th>\n",
       "      <th>BERT</th>\n",
       "      <td>0.558</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.545</td>\n",
       "      <td>0.010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">XGBoost</th>\n",
       "      <th>BERT</th>\n",
       "      <td>0.703</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Count</th>\n",
       "      <td>0.693</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              f1      \n",
       "                            mean   std\n",
       "method               vec              \n",
       "Isolation Forest     BERT  0.334 0.000\n",
       "                     Count 0.333 0.000\n",
       "Local Outlier Factor BERT  0.338 0.001\n",
       "                     Count 0.344 0.002\n",
       "LogReg Stack         BERT  0.680 0.003\n",
       "                     Count 0.617 0.018\n",
       "Logistic Regression  BERT  0.685 0.005\n",
       "                     Count 0.538 0.180\n",
       "OneClassSVM          BERT  0.463 0.023\n",
       "                     Count 0.555 0.002\n",
       "Random Forest        BERT  0.558 0.006\n",
       "                     Count 0.545 0.010\n",
       "XGBoost              BERT  0.703 0.007\n",
       "                     Count 0.693 0.005"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1m.groupby([\"method\", \"vec\"]).agg([\"mean\", \"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.to_markdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
